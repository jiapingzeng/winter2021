\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{relsize}
\usepackage{graphicx}

\title{Math 164 Homework 1}
\date{1/15/2021}
\author{Jiaping Zeng}

\begin{document}
\setstretch{1.35}
\maketitle

\begin{enumerate}
    \item Let $A\in\mathbb{R}^{m\cross n}$ and $\text{rank}(A)=m$. Show that $m\leq n$.\\
          \textbf{Answer}: By definition of matrix rank, $\text{rank}(A)$ is the dimension of the column span of $A$, which cannot exceed the number of columns in the matrix, i.e. $\text{rank}(A)\leq m$. It is also the dimension of the row span which cannot exceed the number of rows in the matrix, i.e. $\text{rank}(A)\leq n$. Therefore $\text{rank}(A)\leq\min(m,n)$. By definition of minimum, if $\text{rank}(A)=m\leq\min(m,n)$, we must have $n\geq m$.
    \item Fill in the blanks in the following $2\cross 2$ matrix \[\begin{bmatrix}-1&?\\?&?\end{bmatrix}\] so that $\norm{A}_\infty=3$.\\
          \textbf{Answer}: \[\begin{bmatrix}-1&0\\-2&0\end{bmatrix}\]
    \item Show that for any two vectors $x,y\in\mathbb{R}^n$, $\abs{\norm{x}-\norm{y}}\leq\norm{x-y}$.\\
          \textbf{Answer}: We have $\abs{\norm{x}-\norm{y}}^2=(\sqrt{\langle x,x\rangle}-\sqrt{\langle y,y\rangle})^2=\langle x,x\rangle+\langle y,y\rangle-\norm{x}\norm{y}$. By Cauchy-Schwarz inequality, $\abs{\langle x,y\rangle}\leq\norm{x}\norm{y}$, then $\abs{\norm{x}-\norm{y}}^2\leq \langle x,x\rangle+\langle y,y\rangle-\norm{x}\norm{y}=(\langle x,x\rangle-\langle x,y\rangle)-(\langle x,y\rangle+\langle y,y\rangle)=\overline{\langle x-y,x\rangle}-\langle x-y,y\rangle$. Since $x,y\in\mathbb{R^n}$, $\overline{\langle x-y,x\rangle}=\langle x-y,x\rangle$. Then $\abs{\norm{x}-\norm{y}}^2\leq\langle x-y,x\rangle-\langle x-y,y\rangle=\langle x-y,x-y\rangle=\norm{x-y}^2$. Since both $\abs{\norm{x}-\norm{y}}$ and $\norm{x-y}$ must be nonnegative, we have $\abs{\norm{x}-\norm{y}}\leq \norm{x-y}$.
    \item Prove that for every positive integer $N$ the following statement holds: For any set of vectors\\$x_1,x_2,\ldots,x_N\in\mathbb{R}^n$, $\norm{x_1+x_2+\cdot+x_N}\leq\norm{x_1}+\norm{x_2}+\cdots+\norm{x_N}$.\\
              \textbf{Answer}: By induction on $N$.\\Base case: $N=2$, we want to show that $\norm{x_1+x_2}\leq\norm{x_1}+\norm{x_2}$, which is true by triangle inequality.\\Inductive step: Suppose that $\norm{x_1+x_2+\cdot+x_{N-1}}\leq\norm{x_1}+\norm{x_2}+\cdots+\norm{x_{N-1}}$, we want to show that $\norm{x_1+x_2+\cdot+x_N}\leq\norm{x_1}+\norm{x_2}+\cdots+\norm{x_N}$. Let $x_m=x_1+\cdots+x_{N-1}$, then $\norm{x_m}=\norm{x_1+\cdots+x_{N-1}}\leq\norm{x_1}+\norm{x_2}+\cdots+\norm{x_{N-1}}$ by inductive hypothesis. Now by substitution and triangle inequality, we have $\norm{x_1+x_2+\cdots+x_N}=\norm{x_m+x_N}\leq\norm{x_m}+\norm{x_N}\leq\norm{x_1}+\norm{x_2}+\cdots+\norm{x_N}$.\\Therefore we have proved the statement by induction.
    \item Prove that the system $Ax=b,A\in\mathbb{R}^{m\cross n}$, has a unique solution if and only if $\text{rank}(A)=\text{rank}([A,b])=n$.\\
          \textbf{Answer}:
          \begin{itemize}
              \item [$\Rightarrow$:] Assume $Ax=b$ has a unique solution, we want to show that $\text{rank}(A)=\text{rank}([A,b])=n$. Since $Ax=b$ has a solution, $b$ must be in the colspan of $A$, so $\text{rank}(A)=\text{rank}([A,b])$. Since the solution is also unique, the columns of $A$ mush be linearly independent, so $\text{rank}(A)=n$. Therefore $\text{rank}(A)=\text{rank}([A,b])=n$.
              \item [$\Leftarrow$:] Assume that $\text{rank}(A)=\text{rank}([A,b])=n$, we want to show that $Ax=b$ has a unique solution. Since $\text{rank}(A)=n$, its columns must be linearly independent. Then since $\text{rank}([A,b])=\text{rank}(A)$, $b$ must be in the colspan of $A$. Therefore $Ax=b$ has a unique solution.
          \end{itemize}
    \item Let $\lambda_1,\cdots,\lambda_n$ be the eigenvalues of the matrix $A\in\mathbb{R}^{n\cross n}$. Show that the eigenvalues of the matrix $\mathbb{I}_n-A$ are $1-\lambda_1,\cdots,1-\lambda_n$. Here $\mathbb{I}_n$ is an identity matrix.\\
          \textbf{Answer}: Since $A$ has $n$ eigenvalues, we can diagonalize $A$ (under the basis of its eigenvectors) to have $\lambda_1,\cdots,\lambda_n$ on the main diagonal. Then, since $\mathbb{I}_n$ has only 1 on the main diagonal, $\mathbb{I}_n-A$ has $1-\lambda_i$ on its main diagonal, therefore its eigenvalues are $1-\lambda_1,\cdots,1-\lambda_n$.
    \item Find the nullspace of \[A=\begin{pmatrix}
                  4 & -2 & 0  \\
                  2 & 1  & -1 \\
                  2 & -3 & 1
              \end{pmatrix}.\]
          \textbf{Answer}: We can find the nullspace by solving $Ax=0$ as follows: \[\begin{pmatrix}
                  4 & -2 & 0  \\
                  2 & 1  & -1 \\
                  2 & -3 & 1
              \end{pmatrix}\begin{pmatrix}
                  x_1 \\x_2\\x_3
              \end{pmatrix}=0\]
          \[\implies 4x_1-2x_2=0, 2x_1+x_2-x_3=0, x_1-3x_2+x_3=0\]
          \[\implies x_1=\frac{x_3}{4}, x_2=\frac{x_3}{2}\]
          Therefore the nullspace is any scalar multiple of the vector $[\frac{1}{4},\frac{1}{2},1]^\top$.
    \item Let $A\in\mathbb{R}^{m\cross n}$ be a matrix. Show that $\mathcal{R}(A)$ is a subspace of $\mathbb{R}^m$ and $\mathcal{N}(A)$ is a subspace of $\mathbb{R}^n$.\\
          \textbf{Answer}: Let $y\in\mathcal{R}(A)$. By definition, there must exist some $x\in\mathbb{R}^n$ such that $Ax=y$. Then the dimension of $y$ is outer dimensions of $A$ and $x$, which gives us $y\in\mathbb{R}^m$. Therefore every vector in $\mathcal{R}(A)$ is in $\mathbb{R}^m$, so $\mathcal{R}(A)$ is a subspace of $\mathbb{R}^m$.
          \\Now let $x\in\mathcal{N}(A)$, then we must have $Ax=0$; since $A\in\mathbb{R}^{m\cross n}$ and the inner dimensions of $A$ and $x$ must match, we have $x\in\mathbb{R}^n$. Therefore every vector in $\mathcal{N}(A)$ is in $\mathbb{R}^n$, so $\mathcal{N}(A)$ is a subspace of $\mathbb{R}^n$.
    \item Let $A\in\mathbb{R}^{m\cross n}$ be a matrix with $A^\top A=\mathbb{I}_n$. Show that $P=AA^\top$ is an orthogonal projection of $\mathcal{R}(A)$.\\
          \textbf{Answer}: Since $A^\top A=\mathbb{I}_n$, we have $P^2=(AA^\top)(AA^\top)=A(A^\top A)A^\top=AA^\top=P$. In addition, $P^*=P^\top=(AA^\top)^\top=(A^\top)^\top A^\top=AA^\top=P$. Now let $y\in\mathcal{R}(A)$, then there must exist some $x\in\mathbb{R}^n$ such that $Ax=y$. Note that $P(y)=AA^\top y=(AA^\top) Ax=A(A^\top A)x=Ax=y\in\mathcal{R}(A)$. Therefore we have $P:\mathcal{R}(A)\rightarrow\mathcal{R}(A)$ where $P$ is both idempotent and self-adjoint, therefore $P$ is an orthogonal projection of $\mathcal{R}(A)$.
    \item Consider the vector space $\mathbb{R}^2$ equipped with the standard $\ell_2$ norm. Let $x=\begin{bmatrix}
                  5 \\6
              \end{bmatrix}$. Find the best approximation \[y=\begin{bmatrix}
                  y_1 \\y_2
              \end{bmatrix}\] to $x$ such that the entries of $y$ satisfy the constraint $y_2=2y_1$.\\
          \textbf{Answer}: let $A=\begin{bmatrix}1\\2\end{bmatrix}$, then $\min_{y_1}\norm{\begin{bmatrix}y_1\\2y_1\end{bmatrix}-\begin{bmatrix}5\\6\end{bmatrix}}_2^2=\min_{y_1}\norm{\begin{bmatrix}1\\2\end{bmatrix}y_2-\begin{bmatrix}5\\6\end{bmatrix}}_2^2=\min\norm{Ay-x}_2^2$. Since $A^\top A=5$ is invertible, the unique minimizer is given by $y_2=A^+x=(A^\top A)^{-1}A^\top x=\dfrac{1}{5}\begin{bmatrix}1&2\end{bmatrix}\begin{bmatrix}5\\6\end{bmatrix}=\dfrac{17}{5}$. Therefore $y=\begin{bmatrix}
              y_1\\2y_2
          \end{bmatrix}=\begin{bmatrix}
              \frac{17}{5}\\\frac{34}{5}
          \end{bmatrix}$.
\end{enumerate}
\end{document}